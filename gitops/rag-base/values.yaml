app: rag-base

dataScienceProjectDisplayName: rag-base
dataScienceProjectNamespace: rag-base

instanceName: rag-base

argocdNamespace: openshift-gitops

vcs:
  uri: https://github.com/davidseve/rag-base.git
  ref: main
  name: alpha-hack-program/rag-base

embeddingsDefaultModel: multilingual-e5-large-gpu

embeddings:
  - name: multilingual-e5-large-gpu
    displayName: multilingual-e5-large GPU
    model: intfloat/multilingual-e5-large
    image: quay.io/atarazana/modelcar-catalog:multilingual-e5-large
    maxModelLen: '512'
    runtime:
      templateName: vllm-serving-template
      templateDisplayName: vLLM Serving Template
      image: quay.io/modh/vllm:rhoai-2.20-cuda
      resources:
        limits:
          cpu: '2'
          memory: 8Gi
        requests:
          cpu: '1'
          memory: 4Gi
    accelerator:
      max: '1'
      min: '1'
      productName: NVIDIA-A10G

models:
  - name: granite-3-3-8b
    displayName: Granite 3.3 8B
    model: ibm-granite/granite-3.3-8b-instruct
    image: quay.io/redhat-ai-services/modelcar-catalog:granite-3.3-8b-instruct
    maxModelLen: '6000'
    runtime:
      templateName: vllm-serving-template
      templateDisplayName: vLLM Serving Template
      image: quay.io/modh/vllm:rhoai-2.20-cuda
      resources:
        limits:
          cpu: '8'
          memory: 24Gi
        requests:
          cpu: '6'
          memory: 24Gi
    accelerator:
      max: '1'
      min: '1'
      productName: NVIDIA-A10G
  - name: mistral-7b-instruct-v0-3
    displayName: Mistral 7B Instruct v0.3
    model: mistralai/Mistral-7B-Instruct-v0.3
    image: quay.io/redhat-ai-services/modelcar-catalog:mistral-7b-instruct-v0.3
    maxModelLen: '4000'
    runtime:
      templateName: vllm-serving-template
      templateDisplayName: vLLM Serving Template
      image: quay.io/modh/vllm:rhoai-2.20-cuda
      resources:
        limits:
          cpu: '8'
          memory: 24Gi
        requests:
          cpu: '6'
          memory: 24Gi
    accelerator:
      max: '1'
      min: '1'
      productName: NVIDIA-A10G
  - name: llama-3-1-8b-w4a16
    displayName: Llama 3.1 8B
    model: RedHatAI/Meta-Llama-3.1-8B-Instruct-quantized.w4a16"
    image: quay.io/redhat-ai-services/modelcar-catalog:llama-3.2-8b-instruct-quantized.w4a16
    maxModelLen: '15000'
    runtime:
      templateName: vllm-serving-template
      templateDisplayName: vLLM Serving Template
      image: quay.io/modh/vllm:rhoai-2.20-cuda
      resources:
        limits:
          cpu: '8'
          memory: 24Gi
        requests:
          cpu: '6'
          memory: 24Gi
    accelerator:
      max: '1'
      min: '1'
      productName: NVIDIA-A10G
    args:
      - --enable-auto-tool-choice
      - --tool-call-parser
      - llama3_json
      - --chat-template
      - /app/data/template/tool_chat_template_llama3.1_json.jinja
milvusApplication:
  name: milvus
  path: gitops/milvus
  targetRevision: main

pipelinesApplication:
  name: pipelines
  path: gitops/pipelines
  targetRevision: main
  vcs: # This is the VCS configuration for the KFP pipelines code
    uri: https://github.com/davidseve/rag-utils.git
    ref: main
    name: alpha-hack-program/rag-utils
    dir: pipelines

routerApplication:
  name: rag-router
  path: gitops/rag-router
  targetRevision: main

documentsConnection:
  name: documents
  displayName: documents
  type: s3
  scheme: http
  awsAccessKeyId: minio
  awsSecretAccessKey: minio123
  awsDefaultRegion: none
  awsS3Bucket: documents
  awsS3Endpoint: minio.ic-shared-minio.svc:9000

pipelinesConnection:
  name: pipelines
  displayName: pipelines
  type: s3
  scheme: http
  awsAccessKeyId: minio
  awsSecretAccessKey: minio123
  awsDefaultRegion: none
  awsS3Bucket: pipelines
  awsS3Endpoint: minio.ic-shared-minio.svc:9000

webuiApplication:
  name: webui
  openaiApiKey: "1234"
  secretKey: "1234"

minio:
  name: minio
  namespace: ic-shared-minio

documentsMilvusConnection:
  name: documents
  collectionName: documents

milvus:
  name: milvus
  username: root
  password: Milvus
  port: '19530'
  host: vectordb-milvus
  database: default
  collectionName: document_chunks

setup:
  image: quay.io/atarazana/hf-cli:latest

mountCaCerts: "false"